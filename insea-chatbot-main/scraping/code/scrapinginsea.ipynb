{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-10-26T05:16:47.472477Z","iopub.status.busy":"2023-10-26T05:16:47.472135Z","iopub.status.idle":"2023-10-26T05:16:53.494463Z","shell.execute_reply":"2023-10-26T05:16:53.493472Z","shell.execute_reply.started":"2023-10-26T05:16:47.472446Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]}],"source":["import json\n","from bs4 import BeautifulSoup\n","import requests\n","from urllib.parse import urlparse, urljoin\n","import re\n","import io\n","import requests\n","from docx import Document\n","import pytesseract\n","import pdf2image\n","from langchain.text_splitter import TokenTextSplitter\n","from sentence_transformers import SentenceTransformer"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-10-26T05:16:53.497103Z","iopub.status.busy":"2023-10-26T05:16:53.496610Z","iopub.status.idle":"2023-10-26T05:16:53.501389Z","shell.execute_reply":"2023-10-26T05:16:53.500336Z","shell.execute_reply.started":"2023-10-26T05:16:53.497076Z"},"trusted":true},"outputs":[],"source":["base_url = 'https://insea.ac.ma'\n","visited_urls = set()"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-10-26T05:16:53.503048Z","iopub.status.busy":"2023-10-26T05:16:53.502646Z","iopub.status.idle":"2023-10-26T05:16:53.534591Z","shell.execute_reply":"2023-10-26T05:16:53.533777Z","shell.execute_reply.started":"2023-10-26T05:16:53.503007Z"},"trusted":true},"outputs":[],"source":["def cleanText(text):\n","    return text.replace(\"\\n\", \" \").replace(\"\\t\", \" \").replace(\"\\r\", \" \").replace(\"\\f\", \" \").strip()"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-10-26T05:16:53.536220Z","iopub.status.busy":"2023-10-26T05:16:53.535826Z","iopub.status.idle":"2023-10-26T05:16:53.545260Z","shell.execute_reply":"2023-10-26T05:16:53.544494Z","shell.execute_reply.started":"2023-10-26T05:16:53.536186Z"},"trusted":true},"outputs":[],"source":["def extract_content_from_document(url):\n","    # Send a GET request to the URL\n","    response = requests.get(url)\n","    # Get the content type of the response\n","    content_type = response.headers['content-type']\n","    # Create a BytesIO object from the response content\n","    content = io.BytesIO(response.content)\n","\n","    if 'application/pdf' in content_type:\n","        # If the content is a PDF, convert it to an image using pdf2image\n","        images = pdf2image.convert_from_bytes(content.read())\n","        # Use pytesseract to OCR the image\n","        text = '\\n'.join(pytesseract.image_to_string(image) for image in images)\n","    elif 'application/vnd.openxmlformats-officedocument.wordprocessingml.document' in content_type:\n","        # If the content is a Word document, use python-docx to read it\n","        doc = Document(content)\n","        text = '\\n'.join(paragraph.text for paragraph in doc.paragraphs)\n","    else:\n","        text = None\n","\n","    return text"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-10-26T05:16:53.546994Z","iopub.status.busy":"2023-10-26T05:16:53.546538Z","iopub.status.idle":"2023-10-26T05:16:53.560609Z","shell.execute_reply":"2023-10-26T05:16:53.559847Z","shell.execute_reply.started":"2023-10-26T05:16:53.546936Z"},"trusted":true},"outputs":[],"source":["def extract_information(url):\n","    # Create a dictionary to store information for the current page\n","    web_page_info = {\n","        \"url\": url,\n","        \"title\": None,\n","        \"text\": None,  # Initialize text as None\n","        \"source_type\": \"webpage\"  # Set the source type to \"webpage\"\n","    }\n","    \n","    documents_info = []\n","    \n","    try:\n","        # Make an HTTP request to the website\n","        r = requests.get(url)\n","        r.raise_for_status()\n","        soup = BeautifulSoup(r.text, 'html.parser')\n","\n","        # Extract the title of the page\n","        title = soup.title.string if soup.title else None\n","        \n","        # Extract paragraphs and their contents\n","        paragraphs = soup.find_all('p')\n","        web_text = ' '.join([re.sub(r'\\s+', ' ', p.get_text().strip()) for p in paragraphs])\n","        web_text_cleane = cleanText(web_text)\n","        \n","        web_page_info[\"title\"] = title\n","        web_page_info[\"text\"] = web_text_cleane\n","        \n","        # Extract links to documents (PDF, Word) and their reference links\n","        for link in soup.find_all('a'):\n","            link_url = link.get('href')\n","            if link_url:\n","                # Join the URL to make it absolute\n","                absolute_link_url = urljoin(base_url, link_url)\n","                if link_url.endswith(('.pdf', '.docx')):  # You can add more extensions\n","                    # Extract reference links \n","                    document_reference_link = absolute_link_url\n","                    # Extract the content of the document\n","                    document_text = cleanText(extract_content_from_document(absolute_link_url))  \n","                    # Create a document dictionary\n","                    document_info = {\n","                        \"url\": document_reference_link,\n","                        \"title\": title,\n","                        \"text\": document_text,\n","                        \"source_type\": link_url.split('.')[-1].upper()  # Extract document type\n","                    }\n","                    documents_info.append(document_info)\n","\n","    except Exception as e:\n","        print(f\"An error occurred while processing {url}: {e}\")\n","        web_page_info = None\n","        documents_info = None\n","        \n","    return web_page_info, documents_info"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-10-26T05:16:53.561872Z","iopub.status.busy":"2023-10-26T05:16:53.561595Z","iopub.status.idle":"2023-10-26T05:16:53.575764Z","shell.execute_reply":"2023-10-26T05:16:53.575000Z","shell.execute_reply.started":"2023-10-26T05:16:53.561849Z"},"trusted":true},"outputs":[],"source":["def crawl(url):\n","    # Skip this URL if it has already been visited\n","    if url in visited_urls:\n","        return\n","\n","    # Parse the URL to compare domains\n","    parsed_url = urlparse(url)\n","    parsed_base_url = urlparse(base_url)\n","    \n","\n","    # Skip this URL if it's not related to the base URL or is from an external domain\n","    if parsed_url.netloc != parsed_base_url.netloc:\n","        return\n","\n","    # Mark this URL as visited\n","    visited_urls.add(url)\n","\n","    try:\n","        # Make an HTTP request to the website\n","        r = requests.get(url)\n","        r.raise_for_status()\n","        \n","        # Parse the HTML content\n","        soup = BeautifulSoup(r.text, 'html.parser')\n","\n","        \n","        # Extract information from the page and save it to the JSON file\n","        web_page_info, documents_info = extract_information(url)\n","        if web_page_info:\n","            if web_page_info[\"text\"]:\n","                data.append(web_page_info)\n","             \n","        if documents_info:\n","            for document_info in documents_info:\n","                if document_info[\"text\"]:\n","                    data.append(document_info)\n","\n","        \n","        # Find all links on the page\n","        links = soup.find_all('a')\n","        \n","        # Follow each link\n","        for link in links:\n","            href = link.get('href')\n","            if href:\n","                absolute_url = urljoin(base_url, href)\n","                # Recursively crawl this link\n","                crawl(absolute_url)\n","\n","    except Exception as e:\n","        print(f\"An error occurred while processing {url}: {e}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data = []\n","crawl(base_url)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
